{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CAIS Compute Cluster","text":"<p>Welcome to the documentation for the Center for AI Safety (CAIS) Compute Cluster, a GPU-accelerated research cluster for Artifical Intelligence/Machine Learning (AI/ML) safety research maintained by the Center for AI Safety. On this site we provide guidance on accessing and using the cluster, from basic usage to advanced workflows.</p> <p>If you have questions not answered here, please consult the FAQ or reach out on Slack or via email at compute@safe.ai.</p>"},{"location":"#cluster-overview","title":"Cluster Overview","text":"<p>The CAIS Compute Cluster consists of 10 GPU nodes (Oracle Cloud bare-metal servers), each with:</p> Feature Description GPU Nodes 10 GPU nodes (Oracle Cloud bare-metal), each with 8\u00d7 NVIDIA A100 80GB GPUs (total 80) CPU Cores Dual 64-core AMD CPUs per node (total 1280 CPU cores across the cluster) Local NVMe SSD 27.2 TB per node (272 TB total system storage) RDMA Network 1,600 Gbit/sec total, providing high-bandwidth and low-latency inter-node communication Operating System Ubuntu 22.04 <p>The cluster is managed using Ansible and Terraform. The cluster uses Slurm for job scheduling, with WekaFS for the shared distributed parallel filesystem.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Q: My job is stuck in the queue. A: Likely the cluster is busy or your resource request is large. Check <code>squeue -u &lt;username&gt;</code> or <code>scontrol show job &lt;jobID&gt;</code> for reasons. During conference deadlines, expect longer wait times. If unsure, ask in Slack #help-desk.</p> <p>Q: How do I request multiple GPUs? A: Request multiple GPUs in Slurm (<code>--gpus-per-node=N</code>).</p> <p>Q: Can I run multiple jobs at once? A: Yes, the scheduler allows multiple concurrent jobs. Fair-share might lower your priority if you exceed usage relative to others.</p> <p>Q: I need software that requires sudo. A: You do not have sudo access. If you are unable to install the software you need yourself through <code>pip</code> or <code>conda</code> please reach out to an admin.</p> <p>For additional questions, ask a question on Slack in the help desk channel.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#access","title":"Access","text":"<ul> <li>Have your Principal Investigator reach out to a CAIS Compute Cluster admin.</li> <li>Once approved, you will receive a username.</li> </ul>"},{"location":"getting-started/#logging-in","title":"Logging in","text":"<p>Access to the CAIS Compute Cluster is provided via Secure Shell (SSH) login. Most Unix-like operating systems provide an SSH client by default that can be accessed by typing the ssh command in a terminal window.</p> <p>To login, open a terminal and type the following command, where  should be replaced with the username you were provided with: <pre><code>$ ssh -i ~/.ssh/id_ed25519 &lt;username&gt;@compute.safe.ai\n$ &lt;username&gt;@cais-login-0:~$\n</code></pre> <p>Upon logging in, you will be connected to a login node (e.g., <code>cais-login-0</code>).</p> <p>If you are having trouble logging in check out our section on SSH.</p> <p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and must not be used for computationally intensive tasks.  Instead, submit those tasks to the scheduler, which will dispatch them to the compute nodes.</p>"},{"location":"getting-started/#getting-compute","title":"Getting compute","text":"<p>Get an interactive session on a node with a single GPU and two CPU cores (the default ratio is 2 CPUs per GPU): <pre><code>$ &lt;username&gt;@cais-login-0:~$ srun --gpus=1 --pty /bin/bash\n$ &lt;username&gt;@compute-permanent-node-535:~$\n</code></pre> Run a command <pre><code>$ &lt;username&gt;@compute-permanent-node-535:~$ nvidia-smi\nTue Feb 25 22:08:12 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:91:00.0 Off |                    0 |\n| N/A   36C    P0             85W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------\n</code></pre></p> <p>How-To Guides</p> <p>After gaining access to the cluster, you can familiarize yourself with it using our How-To Guides. They include information on connecting to the cluster, running jobs, general cluster usage, and more.</p>"},{"location":"guides/","title":"Guides","text":""},{"location":"guides/#overview","title":"Overview","text":"<p>The purpose of the Guides section of the documentation is to provide helpful resources for users who are not familiar with how compute clusters work in general and the CAIS compute cluster in particular. We provide resources for various tools and systems used on the cluster, how to use the cluster to do your work, and how to get the most out of the cluster by configuring it to suit your workflow.</p> <p>The Guides are divided into sections to help you find what you are trying to do.</p>"},{"location":"guides/configuration/how-to-install-miniconda-or-anaconda/","title":"Configuration","text":""},{"location":"guides/configuration/how-to-install-miniconda-or-anaconda/#how-to-install-miniconda-or-anaconda","title":"How to install Miniconda or Anaconda","text":"<p>We suggest installing anaconda or miniconda to facilitate installing many other apps on the server. Here\u2019s how we installed miniconda.</p> <pre><code>curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\n\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n\n./Miniconda3-latest-Linux-x86_64.sh\n# close and reopen shell or say yes to configure the current shell.\n</code></pre> <p>Example of how to install pytorch a popular deep learning library. Similar commands exist for tensorflow etc.</p> <pre><code>pip3 install torch torchvision torchaudio\n</code></pre>"},{"location":"guides/configuration/how-to-resurrect-your-tmux-sessions/","title":"Configuration","text":""},{"location":"guides/configuration/how-to-resurrect-your-tmux-sessions/#how-to-resurrect-your-tmux-sessions","title":"How to resurrect your tmux sessions","text":"<p>If you use tmux we recommend installing tmux-resurrect. This prevents the unfortunate case where if tmux dies so too do all your tabs. With this app you can resurrect your sessions (if you save them).</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/","title":"How to Set Up Claude Code on the Cluster (CCCCC)","text":""},{"location":"guides/configuration/how-to-set-up-claude-code/#what-is-claude-code","title":"What is Claude Code?","text":"<p>Claude Code is Anthropic's command-line coding agent. It runs in your terminal and can read files, write code, and execute commands on your behalf so you can spend your attention where it matters. On this cluster, we provide a shared configuration that teaches Claude the rules of our environment \u2014 Slurm, shared filesystems, GPU etiquette, and common pitfalls \u2014 so it behaves as a responsible cluster citizen from the first session.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#prerequisites","title":"Prerequisites","text":"<ul> <li>An active Claude Pro or Max subscription, or an Anthropic API key</li> <li>SSH access to the cluster</li> </ul>"},{"location":"guides/configuration/how-to-set-up-claude-code/#step-1-install-claude-code","title":"Step 1: Install Claude Code","text":"<pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre> <p>Verify it installed:</p> <pre><code>claude --version\n</code></pre>"},{"location":"guides/configuration/how-to-set-up-claude-code/#step-2-go-to-your-user-directory","title":"Step 2: Go to Your User Directory","text":"<p>Always start from your workspace on the shared filesystem:</p> <pre><code>cd /data/$USER\n</code></pre> <p>This is where all your projects, conda environments, datasets, and outputs should live. Never work out of <code>$HOME</code> for anything large \u2014 home directory quotas are small.</p> <p>You may want to add this to your <code>~/.bashrc</code> so you land here on every login:</p> <pre><code>echo 'cd /data/$USER' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"guides/configuration/how-to-set-up-claude-code/#step-3-authenticate","title":"Step 3: Authenticate","text":"<p>Run <code>claude</code> once to trigger the login flow. It will open a browser URL \u2014 if you're connected via SSH, copy the URL and open it on your local machine to complete authentication.</p> <p>If you're using an API key instead of a subscription:</p> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>Add that export to your <code>~/.bashrc</code> to persist it across sessions.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#step-4-set-up-the-cluster-protocol","title":"Step 4: Set Up the Cluster Protocol","text":"<p>We maintain a shared file at <code>/data/claude/cluster-citizen.md</code> that teaches Claude how to behave on this cluster. To load it automatically every time you run Claude, add this alias to your <code>~/.bashrc</code>:</p> <pre><code>echo 'alias claude=\"claude --append-system-prompt-file /data/claude/cluster-citizen.md\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>That's it. Now every time you type <code>claude</code>, the cluster rules are injected alongside Claude's default instructions and your personal configuration. Your <code>~/.claude/CLAUDE.md</code> and any project-level settings continue to work normally.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#verify-its-working","title":"Verify it's working","text":"<p>Start a session and ask:</p> <pre><code>&gt; What partitions are available on this cluster?\n</code></pre> <p>Claude should respond with <code>cais</code>, <code>cais_cpu</code>, <code>tamper_resistance</code>, and <code>tamper_resistance_cpu</code> without you needing to tell it.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#step-5-optional-add-your-own-claudemd","title":"Step 5 (Optional): Add Your Own CLAUDE.md","text":"<p>The cluster protocol covers shared rules. For your own project-specific context, create a <code>CLAUDE.md</code> in your project directory:</p> <pre><code>cd /data/$USER/my-project\ncat &gt; CLAUDE.md &lt;&lt; 'EOF'\n# My Project\n\n- Python 3.11, PyTorch 2.x\n- Conda env: myenv\n- Tests: pytest tests/\n- Data lives in /data/$USER/datasets/imagenet/\nEOF\n</code></pre> <p>Claude loads this automatically when you run <code>claude</code> from that directory. It stacks with the cluster protocol \u2014 both apply.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#usage-tips","title":"Usage Tips","text":""},{"location":"guides/configuration/how-to-set-up-claude-code/#use-tmux-for-session-persistence","title":"Use tmux for session persistence","text":"<p>You're SSHing into the cluster. If your connection drops, your Claude Code session dies \u2014 unless you're inside tmux. Always start Claude inside a tmux session:</p> <pre><code># Start a new tmux session\ntmux new -s claude\n\n# Now start Claude as normal\nclaude\n\n# If your SSH drops, reconnect and reattach:\ntmux attach -t claude\n</code></pre> <p>Your conversation, running processes, and Claude's context are all preserved.</p> <p>tmux is also required if you want to use agent teams \u2014 Claude Code's feature for spawning multiple agents working in parallel in separate panes. Without tmux, agent teams still work but only in a single-terminal mode where you cycle between agents with <code>Shift+Up/Down</code>.</p> <p>Quick tmux reference:</p> Action Keys Detach (leave running) <code>Ctrl+b</code> then <code>d</code> List sessions <code>tmux ls</code> Reattach <code>tmux attach -t claude</code> Kill session <code>tmux kill-session -t claude</code>"},{"location":"guides/configuration/how-to-set-up-claude-code/#always-use-slurm-for-heavy-work","title":"Always use Slurm for heavy work","text":"<p>Claude knows this rule, but reinforce it by telling Claude what you're doing:</p> <pre><code>&gt; I need to train a model on 2 GPUs for ~6 hours. Write me an sbatch script.\n</code></pre> <p>Claude will use the correct partition names and resource flags for this cluster.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#use-conda-not-pip-user","title":"Use conda, not pip --user","text":"<p>All Python environments should be conda envs stored under <code>/data/$USER/</code>. If you don't have miniconda yet, follow the How to Install Miniconda or Anaconda guide.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#reporting-mistakes","title":"Reporting mistakes","text":"<p>If Claude does something wrong that other users should be protected from \u2014 like trying to run training on the login node, or installing packages to the wrong location \u2014 report it to Jason Lim via Slack. We maintain a shared lessons file that gets updated so every Claude session on the cluster learns from past mistakes.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#quick-reference","title":"Quick Reference","text":"What Command Start Claude <code>claude</code> Check Slurm queue <code>squeue -u $USER</code> Interactive GPU session <code>salloc --partition=cais --gres=gpu:1 --time=01:00:00</code> Submit batch job <code>sbatch job.slurm</code> Cancel a job <code>scancel &lt;jobid&gt;</code> Check GPU availability <code>sinfo -p cais --format=\"%N %G %t\"</code> Activate conda env <code>conda activate myenv</code>"},{"location":"guides/configuration/how-to-set-up-claude-code/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/configuration/how-to-set-up-claude-code/#command-not-found-claude","title":"\"command not found: claude\"","text":"<p>The install script may not have added Claude to your PATH. Try opening a new shell, or run <code>source ~/.bashrc</code>. If it still fails, check that <code>~/.claude/bin</code> (or wherever the installer placed it) is in your PATH.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#claude-doesnt-know-about-the-cluster","title":"Claude doesn't know about the cluster","text":"<p>The alias isn't set. Run <code>alias claude</code> to check. If it doesn't show the <code>--append-system-prompt-file</code> flag, re-run the Step 3 command.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#authentication-fails-over-ssh","title":"Authentication fails over SSH","text":"<p>Copy the URL Claude prints and open it in a browser on your local machine. Complete the login there. The CLI will detect the auth automatically.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#claude-tries-to-run-training-on-the-login-node","title":"Claude tries to run training on the login node","text":"<p>Tell it explicitly: \"This must go through Slurm.\" If the cluster protocol is loaded, Claude should already know this \u2014 if it doesn't, check that your alias is set correctly.</p>"},{"location":"guides/configuration/how-to-set-up-claude-code/#home-directory-is-full","title":"Home directory is full","text":"<p>Likely caused by pip/conda installing to <code>$HOME</code>. Move your conda installation to <code>/data/$USER/miniconda3</code> and set <code>HF_HOME</code> as described above.</p> <p>Contact the administrator if none of the solutions are satisfactory.</p>"},{"location":"guides/configuration/switch-your-shell/","title":"Configuration","text":""},{"location":"guides/configuration/switch-your-shell/#how-to-switch-your-shell","title":"How to switch your shell","text":"<p>Add the following to the end of your <code>.bashrc</code> file to which shell you'd like to run. The different shells are by default installed into <code>/usr/bin/</code>. <pre><code># To switch to zsh\nif [ \"$SHELL\" != \"/usr/bin/zsh\" ]\nthen\n    export SHELL=\"/usr/bin/zsh\"\n    exec /usr/bin/zsh\nfi\n</code></pre></p> <p>If your favorite shell is not installed on the system just ask an admin to add it for you.</p>"},{"location":"guides/connect/cli/","title":"Connect to the cluster","text":""},{"location":"guides/connect/cli/#connect-with-the-command-line","title":"Connect with the command line","text":"<p>Use your private key to authenticate when connecting to a running instance via SSH. The instance has an SSH key associated with it, provided when that instance was created. The private key must correspond to that SSH key.</p> Connect to the login node with SSH <pre><code>ssh -i &lt;path_to_your_private_key&gt; &lt;username&gt;@&lt;ip_address&gt;\n</code></pre> <p>Examples:</p> Ed25519RSA <pre><code>ssh -i ~/.ssh/id_ed25519 john_smith@compute.safe.ai\n</code></pre> <pre><code>ssh -i ~/.ssh/id_rsa john_smith@compute.safe.ai\n</code></pre>"},{"location":"guides/connect/juypter/","title":"Connect to the cluster","text":""},{"location":"guides/connect/juypter/#how-to-connect-with-jupyter","title":"How to connect with Jupyter","text":"<p>Prerequisites</p> <ul> <li>You can SSH into the cluster (e.g. ssh username@compute.safe.ai).</li> <li>Anaconda (or another Python environment) installed.  </li> </ul>"},{"location":"guides/connect/juypter/#connect-to-the-cluster_1","title":"Connect to the Cluster","text":"<pre><code>ssh -i ~/.ssh/id_ed25519 &lt;username&gt;@compute.safe.ai\n</code></pre>"},{"location":"guides/connect/juypter/#install-or-confirm-jupyter-is-installed","title":"Install or Confirm Jupyter is Installed","text":"<p>Make sure Jupyter is available in your environment: <pre><code># If using conda\nconda install -c anaconda jupyter\n\n# Alternatively, using pip\npip install jupyter\n</code></pre></p>"},{"location":"guides/connect/juypter/#request-an-interactive-cpu-node","title":"Request an Interactive CPU Node","text":"<p>From the login node, run: <pre><code>srun --pty bash\n</code></pre> This gives you an interactive shell on one of the compute nodes. Note which node you landed on (it might be something like <code>compute-permanent-node-###</code>).</p>"},{"location":"guides/connect/juypter/#start-jupyter-on-the-compute-node","title":"Start Jupyter on the Compute Node","text":"<p>Once you\u2019re on the compute node, set up a random port and start Jupyter: <pre><code># Unset this variable if set, to avoid issues\nunset XDG_RUNTIME_DIR\n\n# Export a random port above 1024\nexport NODEPORT=$(( $RANDOM + 1024 ))\necho \"Using port $NODEPORT\"\n\n# Launch Jupyter without opening a browser\njupyter notebook --no-browser --port=$NODEPORT\n</code></pre></p> <p>Keep track of:</p> <ol> <li>The port number you see in <code>$NODEPORT</code>.  </li> <li>The notebook URL that Jupyter prints out (something like <code>http://localhost:19303/?token=...</code>).</li> </ol> <p>Leave this terminal open while Jupyter runs.</p>"},{"location":"guides/connect/juypter/#set-up-ssh-tunneling-from-your-local-machine","title":"Set Up SSH Tunneling from Your Local Machine","text":"<p>Open a new terminal on your local machine (not the compute node) and run: <pre><code># Example: If NODEPORT=19303 and your node is \"compute-permanent-node-123\"\nexport NODEPORT=19303\n\nssh -t -t cais_cluster -L ${NODEPORT}:localhost:${NODEPORT} \\\n    ssh -N compute-permanent-node-123 \\\n    -L ${NODEPORT}:localhost:${NODEPORT}\n</code></pre></p> <p>Explanation:</p> <ul> <li>The first SSH command connects you to the cluster\u2019s login node.  </li> <li>The second SSH command (nested inside) connects you from the login node to the compute node, while setting up port forwarding so that traffic on localhost:$NODEPORT flows to the Jupyter process.</li> </ul>"},{"location":"guides/connect/juypter/#open-jupyter-notebook-in-your-browser","title":"Open Jupyter Notebook in Your Browser","text":"<p>Finally, open your preferred browser on your local machine and paste in the notebook link you copied from step 4. For example: <pre><code>http://localhost:19303/?token=cb2b708e5468268ase8c46448fc28e78bd049a977cdcbd65d1\n</code></pre> That\u2019s it! You should now see the Jupyter Notebook interface running on the compute node, but accessible in your local browser. You can run notebooks as normal, including CPU- or GPU-based workloads as permitted by the node you requested with srun.</p> <p>Troubleshooting</p> <ul> <li>Port Already in Use: If you see an error about the port already in use, run the steps again with a different random port or specify export NODEPORT=##### manually.  </li> <li>SSH Config: If your SSH setup differs, adjust commands to use an alias if you have one.  </li> <li>Environment Issues: Ensure that your Python environment is loaded before running jupyter notebook.</li> </ul>"},{"location":"guides/connect/vscode/","title":"Connect to the cluster","text":""},{"location":"guides/connect/vscode/#how-to-connect-with-vscode","title":"How to connect with VSCode","text":"<p>This guide explains how to use Visual Studio Code (VS Code) for remote development on the CAIS Compute Cluster. It covers performance considerations, recommended settings, and the two primary remote extensions: <code>Remote - SSH</code> and <code>Remote - Tunnels</code>.</p>"},{"location":"guides/connect/vscode/#remote-extensions-overview","title":"Remote Extensions Overview","text":"<p>There are two primary ways to develop remotely using VS Code:</p> <pre><code>1.  Remote - SSH Extension\n2.  Remote - Tunnels Extension\n</code></pre> <p>Both deliver nearly identical development experiences in VS Code; they differ only in how they connect to the server. Choose whichever best fits your workflow.</p>"},{"location":"guides/connect/vscode/#using-the-remote-ssh-extension","title":"Using the Remote - SSH Extension","text":"<p>Instructions</p> <ol> <li>Install the extension<ul> <li>In VS Code, open the Extensions panel (Ctrl+Shift+X or \u2318+Shift+X) and search for Remote - SSH.</li> </ul> </li> <li>Configure SSH<ul> <li>Follow the basic SSH setup to ensure you can ssh to your cluster (e.g., ssh cais_cluster_alias or ssh username@compute.safe.ai).</li> <li>If you have an ~/.ssh/config file, VS Code will detect any hosts listed there.</li> </ul> </li> <li>Enable remote.SSH.remoteServerListenOnSocket<ul> <li>Open VS Code Settings (Ctrl+, or \u2318+,) and search for remote.SSH.remoteServerListenOnSocket.</li> <li>Check (enable) this option. It\u2019s disabled by default.</li> <li>This ensures a more stable, socket-based communication between your local VS Code client and the cluster.</li> </ul> </li> <li>Connect to the Cluster<ul> <li>Press F1 (or Ctrl+Shift+P/\u2318+Shift+P) to open the Command Palette.</li> <li>Type Remote-SSH: Connect to Host..., and select your configured SSH host (e.g., cais_cluster_alias).</li> </ul> </li> <li>Open or Create a Project<ul> <li>Once connected, you can open a folder on the remote machine or create a new one.</li> <li>Install any necessary extensions on the remote side when prompted (VS Code will ask to install or \u201creload\u201d them).</li> </ul> </li> </ol> <p>That\u2019s it! You can now develop on the cluster as if it were a local environment, with code-completion, integrated terminal, debugging, etc.</p>"},{"location":"guides/connect/vscode/#using-the-remote-tunnels-extension","title":"Using the Remote - Tunnels Extension","text":"<p>The Remote - Tunnels extension allows you to run VS Code in the browser or desktop client without needing direct SSH from your local VS Code. It can be more convenient for connecting to an interactive Slurm session on a compute node.</p> <p>Instructions</p> <ol> <li>Install the extension<ul> <li>In VS Code, open Extensions and search for Remote - Tunnels.</li> </ul> </li> <li>SSH into the Cluster<ul> <li>From a terminal (outside of VS Code), ssh cais_cluster (or ssh @compute.safe.ai). <li>We strongly encourage running commands from your /data/ directory to ensure the correct privacy permissions for any folders VS Code creates. <li>Start the Tunnel<ul> <li>Once logged in, run the code tunnel command (or similar command that the Remote - Tunnels documentation provides).</li> <li>You\u2019ll see instructions and a URL to open the remote environment.</li> </ul> </li> <li>Open VS Code or Browser<ul> <li>If you\u2019re using the desktop VS Code client, enter the provided code to authenticate and connect.</li> <li>If you\u2019re using the browser, open the provided URL in your web browser to access the remote environment.</li> </ul> </li> <li>(Optional) Interactive Node<ul> <li>If you plan to work on a compute node rather than the login node, you can request an interactive session (srun --pty bash) and then run the code tunnel command there. This ensures your code execution is on the compute node itself.</li> </ul> </li>"},{"location":"guides/connect/what-is-ssh/","title":"Connect to the cluster","text":""},{"location":"guides/connect/what-is-ssh/#what-is-ssh","title":"What is SSH?","text":""},{"location":"guides/connect/what-is-ssh/#the-secure-shell-ssh-protocol","title":"The Secure Shell (SSH) protocol","text":"<p>SSH is a cryptographic network protocol used for secure access to a networked computer. The data transmitted between the client and server is protected with strong encryption algorithms, ensuring privacy and data integrity.</p> <p>All compute nodes on the cluster run SSH servers for remote access. These servers authenticate using SSH keys.</p>"},{"location":"guides/connect/what-is-ssh/#about-ssh-keys","title":"About SSH keys","text":"<p>For improved security, we use public-key cryptography for authentication instead of a password. This is also known as SSH key authentication. This method requires that you provide your own public/private key pair.</p> <p>Your public key is stored in your home directory <code>~/.ssh/authorized_keys</code>. Its corresponding private key is stored locally on the computer that you use to access your instance. SSH access is granted by verifying that your private key corresponds to the public key. As an analogy, think of a public key as a lock that only your private key can unlock.</p> <p>Tip</p> <p>Public/private key pairs are used for many purposes, not only SSH. For example, they are used in file encryption, digital signatures, and cryptocurrency transactions.</p>"},{"location":"guides/connect/what-is-ssh/#ssh-clients","title":"SSH clients","text":"<p>An SSH client is used to remotely connect to an instance. Nearly all computers have an SSH client installed by default. To test if you have one installed, enter the following command in your CLI shell:</p> <pre><code>ssh -v\n</code></pre> <p>You should see a list of usage options. If you see an output indicating that the command was not found, install an SSH client. Two popular options are OpenSSH and Putty.</p>"},{"location":"guides/connect/what-is-ssh/#create-an-ssh-key-pair","title":"Create an SSH key pair","text":"<p>If you already have a public/private key pair of a supported format, you can use that pair instead of generating a new one.</p> <p>Follow the steps below to generate a key pair with a command-line tool called <code>ssh-keygen</code>. This tool is pre-installed on nearly all operating systems.</p> <p>Use any supported format for your key pair. The tabs below contain instructions for generating a key pair with either the Ed25519 algorithm or RSA algorithm.</p> ED25519RSA <pre><code># Generate the key pair \nssh-keygen -t ed25519 -C \"your_email@example.com\"\n# If prompted to enter a filename, press `Enter` to save the key pair to the default location (`~/.ssh/`). \n# When prompted to enter a passphrase, you can optionally do so to increase secruity.\n</code></pre> <pre><code># Confirm that your key pair was created\nls ~/.ssh\n</code></pre> <pre><code># View your public key\ncat ~/.ssh/id_ed25519.pub\n</code></pre> <pre><code># Generate the key pair\nssh-keygen -C \"your_email@example.com\"\n# If prompted to enter a filename, press `Enter` to save the key pair to the default location (`~/.ssh/`). \n# When prompted to enter a passphrase, you can optionally do so to increase secruity.\n</code></pre> <pre><code># Confirm that your key pair was created\nls ~/.ssh\n</code></pre> <pre><code># View your public key\ncat ~/.ssh/id_rsa.pub\n</code></pre> Warning <p>With any public/private key pair that you generate, your public key can be shared freely. Your private key should be just that\u2014private! Take care not to share it with others, or in public repositories such as in GitHub.</p>"},{"location":"guides/running-jobs/cpu-jobs/","title":"Running jobs","text":""},{"location":"guides/running-jobs/cpu-jobs/#cpu-jobs","title":"CPU jobs","text":""},{"location":"guides/running-jobs/cpu-jobs/#interactive-cpu-jobs","title":"Interactive CPU jobs","text":"<p>You can request an interactive CPU job with the <code>srun</code> Slurm command. </p>"},{"location":"guides/running-jobs/cpu-jobs/#available-cpu-partitions","title":"Available CPU Partitions","text":"<p>When submitting CPU jobs, specify one of the following partitions using --partition (or -p) as the first argument     \u2022   cais_cpu     \u2022   schmidt_sciences_cpu     \u2022   tamper_resistance_cpu</p> <pre><code># Request 1 cpu on 1 node\n# cais partition\nsrun --partition=cais_cpu --pty /bin/bash\n\n# schmidt_sciences partition\nsrun --partition=schmidt_sciences_cpu --pty /bin/bash\n\n# tamper_resistance_cpu partition\nsrun --partition=tamper_resistance_cpu --pty /bin/bash\n\n# Exit from the compute node to request a new node\nexit # or hit ctrl+d\n</code></pre>"},{"location":"guides/running-jobs/cpu-jobs/#batch-cpu-jobs","title":"Batch CPU jobs","text":"<p>You can schedule a batch with the <code>sbatch</code> Slurm command which will place the job onto the job queue. The suggested workflow is to debug and get things working with <code>srun</code> and then transition into putting jobs into the queue. Here is an example <code>sbatch</code> script: <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=10:00\n#SBATCH --job-name=Example \n\ngcc --version  # prints out the gcc version.\n\npython --version  # prints out the python version.  Replace this with a python call to whatever file.\n\nsleep 5\n</code></pre></p>"},{"location":"guides/running-jobs/cpu-jobs/#submit-the-job","title":"Submit the job","text":"<p>Once your batch script is ready, you can submit it to the scheduler with: <pre><code>$ sbatch cpu_job.sh\nSubmitted batch job 123456\n</code></pre> Upon success, <code>sbatch</code> will return the ID it has assigned to the job (in this example, <code>123456</code>)</p>"},{"location":"guides/running-jobs/cpu-jobs/#check-the-job","title":"Check the job","text":"<p>After submission, your job will enter the queue in the <code>PENDING</code> state. When resources become available and your job has sufficient priority, it will start running (<code>RUNNING</code> state). If it completes successfully, it will transition to <code>COMPLETED</code>. Otherwise, it may show <code>FAILED</code> or another terminal state.</p> <p>Use <code>squeue</code> to check the status of your job: <pre><code>$ squeue -u $USER\n     JOBID   PARTITION    NAME      USER ST   TIME  NODES NODELIST(REASON)\n    123456     compute Example &lt;username&gt; R    0:12    1   compute-permanent-node-535\n</code></pre></p>"},{"location":"guides/running-jobs/cpu-jobs/#slurm-output-file","title":"Slurm output file","text":"<p>By default, Slurm writes any output from your script to a file named slurm-.out. You can list the contents of that output file as follows (using the same job ID that was returned by <code>sbatch</code>): <pre><code>$ cat slurm-123456.out\ngcc (GCC) 9.4.0\nPython 3.12.2\n</code></pre> <p>You can customize the output file name by addin gdirectives (e.g., <code>#SBATCH --output=myjob.out</code>) in your <code>sbatch</code> script or specifying them at the command line.</p>"},{"location":"guides/running-jobs/gpu-jobs/","title":"Running jobs","text":""},{"location":"guides/running-jobs/gpu-jobs/#gpu-jobs","title":"GPU jobs","text":""},{"location":"guides/running-jobs/gpu-jobs/#interactive-gpu-jobs","title":"Interactive GPU jobs","text":"<p>Available partitions - similar to cpu partition - cais - schmidt_sciences - tamper_resistance</p> <p>You can request an interactive GPU job with the <code>srun</code> Slurm command. <pre><code># Request 1 GPU on 1 node (2 CPU cores are allocated per GPU by default)\nsrun --partition=cais --gpus=1 --pty /bin/bash\nsrun --partition=cais --gres=gpu:1 --pty /bin/bash # also works\n\n# schmidt_sciences partition\nsrun --partition=schmidt_sciences --gpus=1 --pty /bin/bash\nsrun --partition=schmidt_sciences --gres=gpu:1 --pty /bin/bash\n\n# tamper_resistance partition\nsrun --partition=tamper_resistance --gpus=1 --pty /bin/bash\nsrun --partition=tamper_resistance --gres=gpu:1 --pty /bin/bash\n\n\n# Exit from the compute node to request a new node\nexit  # or hit ctrl+d\n</code></pre></p>"},{"location":"guides/running-jobs/gpu-jobs/#batch-gpu-jobs","title":"Batch GPU jobs","text":"<p>You can schedule a batch GPU job with the sbatch Slurm command, which will queue your job for execution on an available GPU node. As with CPU jobs, we suggest debugging your job interactively (using srun) before submitting a batch job. Here is an example sbatch script for a GPU job: <pre><code>#!/bin/bash\n#SBATCH --partition=&lt;your-partition&gt;\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=2  # 2 CPUs per GPU (default ratio)\n#SBATCH --gpus=1\n#SBATCH --time=10:00\n#SBATCH --job-name=GPU_Example\n\n# Check that the GPU is available\nnvidia-smi\n\n# Run your GPU-accelerated application\npython --version  # Replace this with your actual GPU-enabled command\n\nsleep 5\n</code></pre></p> <p>Once the script is written, you can submit it to the scheduler with the <code>sbatch</code> command. Upon success, <code>sbatch</code> will return the ID it has assigned to the job <pre><code>$ &lt;username&gt;@cais-login-0:~$ sbatch gpu_job.sh \nSubmitted batch job 186269\n</code></pre></p>"},{"location":"guides/running-jobs/gpu-jobs/#check-the-job","title":"Check the job","text":"<p>Once submitted, the job enters the queue in the PENDING state. When resources become available and the job has sufficient priority, an allocation is created for it and it moves to the RUNNING state. If the job completes correctly, it goes to the COMPLETED state, otherwise, its state is set to FAILED.</p> <p>You'll be able to check the status of your job and follow its evolution with the <code>squeue -u $USER</code> command:</p> <pre><code>$ &lt;username&gt;@cais-login-0:~$ squeue -u $USER\n     JOBID PARTITION     NAME          USER        ST   TIME    NODES  NODELIST(REASON)\n      123    compute     GPU_Example   &lt;username&gt;  R    0:12    1      compute-permanent-node-535\nThe scheduler will automatically create an output file that will contain the result of the commands run in the script file. That output file is names slurm-&lt;jobid&gt;.out by default, but can be customized via submission options. In the above example, you can list the contents of that output file with the following commands:\n\n$ &lt;username&gt;@cais-login-0:~$ cat slurm-186269.out\nTue Feb 25 23:06:42 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:91:00.0 Off |                    0 |\n| N/A   36C    P0             85W /  400W |       1MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nPython 3.12.2\n</code></pre>"},{"location":"guides/running-jobs/notifications/","title":"Configuration","text":""},{"location":"guides/running-jobs/notifications/#how-to-configure-job-notifications","title":"How to configure job notifications","text":"<p>We have configured SLURM for sending email and Slack notifications for various job stages (begin, fail, requeue, complete, etc) from the <code>do-not-reply@safe.ai</code> email address (check spam). It is also capable of interfacing with other notification platforms, so if you would like us to configure it for another platform, let us know and we will look into it. We are using the goslmailer for the notifications.</p> <p>You can add notifications to your job by adding the following lines to your SBATCH file: <pre><code>#SBATCH --mail-user=mailto:{email},slack:{slack-member-id}\n#SBATCH --mail-type=ALL\n</code></pre> Feel free to replace <code>{email}</code> with one of your choice and <code>{slack-member-id}</code> with your personal one for the CAIS Compute Cluster workspace (how to find your member id). If you would only like to use Slack or email for notifications and not the other, you can do this by only including that service in your sbatch.</p> <p>Example: <pre><code>#SBATCH --mail-user=mailto:{email}\n#SBATCH --mail-type=ALL\n</code></pre> You can also configure the messages with all the mail-type options found in SLURM by default.</p>"},{"location":"guides/running-jobs/what-is-slurm/","title":"Running jobs","text":""},{"location":"guides/running-jobs/what-is-slurm/#what-is-slurm","title":"What is Slurm?","text":"<p>The CAIS Compute Cluster uses Slurm, an open-source resource manager and job scheduler.</p> <p>Slurm supports a variety of job submission techniques. By accurately requesting the resources you need, you\u2019ll be able to get your work done.</p>"},{"location":"guides/running-jobs/what-is-slurm/#slurm-commands","title":"Slurm commands","text":"<p>Slurm allows requesting resources and submitting jobs in a variety of ways. The main Slurm commands to submit jobs are listed in the table below:</p> Command Description Behavior <code>salloc</code> Request resources and allocates them to a job Starts a new shell, but does not execute anything <code>srun</code> Request resources and runs a command on the allocated compute node(s) Blocking command: will not return until the job ends <code>sbatch</code> Request resources and runs a script on the allocated compute node(s) Asynchronous command: will return as soon as the job is submitted <p>For usage examples, check out requesting CPU jobs and GPU jobs.</p>"},{"location":"guides/running-jobs/what-is-slurm/#how-we-configured-slurm","title":"How we configured Slurm","text":"<p>The priority of your jobs in the Slurm queue is determined based on the size of the job, the time you\u2019ve been waiting and your group\u2019s previous usage relative to other groups (\u201cfair share\u201d, in Slurm\u2019s terminology).</p> <p>You should aim to request the minimum number of GPUs needed to run your job efficiently. Requesting more GPUs than needed will result in your group\u2019s priority declining faster than otherwise, which means you will need to wait longer in the queue to run jobs in future. You can check the utilisation of GPU cores and memory to see if jobs are running efficiently using: <pre><code>srun --jobid=&lt;JOBID&gt; nvidia-smi\n</code></pre></p>"},{"location":"guides/software/what-is-apptainer/","title":"Software","text":"<p>Apptainer support is currently in Beta.</p> <p>If you encounter issues please reach out via Slack for support.</p>"},{"location":"guides/software/what-is-apptainer/#what-is-apptainer","title":"What is Apptainer?","text":"<p>Apptainer is a container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. It's like Docker, but for HPC systems.</p> <p>There is no setup required but Apptainer is only available on compute nodes.</p>"},{"location":"guides/software/what-is-apptainer/#how-to-import-an-image","title":"How to import an image","text":"<ol> <li>Select a Docker image on Docker Hub or another registry.  </li> <li>Pull and convert it with <pre><code># request a CPU job\nsrun -p {partition} -c 1 --pty bash\n# example\napptainer pull docker://{repository}/{image}:{tag}      \n# import ubuntu image  \napptainer pull docker://ubuntu:24.04   \n</code></pre></li> <li>Apptainer downloads the layers and transparently bundles them into a <code>.sif</code> file.</li> </ol>"},{"location":"guides/software/what-is-apptainer/#how-to-build-an-image","title":"How to build an image","text":"<p>Start by connecting to a compute node; a single CPU will be enough but requesting more can speed up the build. <pre><code>srun -p {partition} -c 1 --pty bash\n</code></pre></p> <p>Next, make a directory on your home dir for container building: <pre><code>mkdir -p \"/data/$USER/apptainer/build/\"\ncd \"/data/$USER/apptainer/build/\"\n</code></pre></p> <p>Now pull an existing image to a new folder for building the new image. We will use a stock docker image with cuda 12.8 support. We name this image <code>pytorch_container</code>, but that can be replaced as you like:</p> <p>this step pulls the docker image <pre><code>apptainer build --sandbox pytorch_container docker://nvidia/cuda:12.8.1-cudnn-devel-ubuntu24.04\n</code></pre> After getting the base image, we need to start up the container in <code>writable</code> mode to install the software we want: <pre><code># --writable allows for writing to the container, necessary for installing new software\n# --no-home prevents issues with NFS home directories and writable containers\n# --nv allows for gpu support\n# --fakeroot is required for installing packages in the container as root, in particular using apt\napptainer shell --no-home --nv --writable --fakeroot pytorch_container\n</code></pre> Now we should have a shell inside the container, where we will install software. In general the flow should be apt packages first, then conda packages, and then finally pip packages. <pre><code># make sure package lists are current\napt update -y &amp;&amp; apt upgrade -y\n\n# install generally useful packages\napt install -y git wget vim\napt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# create a location inside the container for a miniconda installation\nmkdir /conda_tmp\ncd /conda_tmp\n\n# install miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -p /conda_tmp/mc3\nrm Miniconda3-latest-Linux-x86_64.sh\neval \"$(/conda_tmp/mc3/bin/conda 'shell.bash' 'hook')\"\n\n# install pytorch with cuda 12.8 support\nconda install -y -c pytorch -c nvidia pytorch pytorch-cuda=12.8 torchvision torchaudio\n\n# install any other python packages needed\n\n# exit the container\nexit\n</code></pre> After we install software in the container in writable mode, we need to build the container to an image so that we can use it for running software in readable mode. To do this, outside the container run, this step will take a while: <pre><code>apptainer build pytorch_container.sif pytorch_container\n</code></pre> After the build has finished, we should have a single image file that is the whole container. Typically this file is quite large (~10G). <pre><code># clear the apptainer cache which takes up a lot of space\nrm -r /data/$USER/.apptainer/cache\n\n# copy the image\nmkdir /data/$USER/apptainer/build/\ncp pytorch_container.sif /data/$USER/apptainer/pytorch_container.sif\n\n# (optional) clean up build directory \ncd ~\nrm -r /data/$USER/apptainer/build/\n</code></pre> After this step has finished, logout from the compute node: <pre><code>exit  # logout\n</code></pre></p>"},{"location":"guides/software/what-is-apptainer/#how-to-run-a-container","title":"How to run a container","text":"<p>There are two ways to use the container, interactive and non-interactive. We will go through examples of both.</p>"},{"location":"guides/software/what-is-apptainer/#interactive-jobs","title":"Interactive jobs","text":"<p>First, allocate a compute node in interactive mode: <pre><code>srun -p {partition} -G 1 --pty bash\n</code></pre> Next, enter the container and activate the python environment we installed: <pre><code># enter the container:\n# --nv is needed for gpu access\napptainer shell --nv /data/$USER/apptainer/pytorch_container.sif\n# activate miniconda environment\neval \"$(/conda_tmp/mc3/bin/conda 'shell.bash' 'hook')\"\n</code></pre> A simple test of our install is to run the python shell and see if we have GPU access: <pre><code>python\nimport torch\na = torch.ones(1)\na.cuda()\n</code></pre> If this works and a is on the GPU, all is well!</p>"},{"location":"guides/software/what-is-apptainer/#batch-jobs","title":"Batch jobs","text":"<p>To run the container in batch mode we will need two scripts: the first will be an sbatch script to launch the job on the cluster, and the second will be a script that runs inside the container environment, containing all of the python code.</p> <p>First, create a <code>test_pytorch.py</code> file to test our python environment: <pre><code>import torch\na = torch.ones(1)\nprint(a.cuda())\n</code></pre></p> <p>Then, create a <code>test_pytorch.sh</code> script to run inside the container: <pre><code>#!/bin/bash\neval \"$(/conda_tmp/mc3/bin/conda 'shell.bash' 'hook')\"\ncd ~\npython test_pytorch.py\n</code></pre></p> <p>Next, create an <code>test_pytorch.sbatch</code> script to launch our job, which should look like: <pre><code>#!/bin/bash\n#SBATCH --partition={gpu-enabled-partition}\n#SBATCH --gpus=1\n\ncd ~\napptainer exec --nv /data/$USER/apptainer/pytorch_container.sif bash test_script.sh\n</code></pre></p> <p>Finally, launch the job <pre><code>sbatch test_pytorch.sbatch\n</code></pre></p>"},{"location":"storage/","title":"Storage","text":"<p>The CAIS Compute Cluster is equipped with a distributed parallel filesystem (Weka) deployed across all compute nodes. In the CCC, a single filesystem is mounted at <code>/data</code>. This filesystem houses:</p> <ul> <li>User home directories</li> <li>Shared group directories</li> <li>Models</li> <li>Datasets</li> </ul> <p>Because Weka uses the local SSDs from compute nodes as part of the distributed filesystem, you cannot use a node\u2019s local SSDs directly in a job or for storing data manually.</p>"},{"location":"storage/#user-home-directories","title":"User Home Directories","text":"<ul> <li>Location: <code>/data/&lt;username&gt;</code></li> <li>Quota: 500 GB per user</li> </ul> <p>Your home directory is the primary space for storing personal scripts, data, and other files. If you approach your quota, you may need to delete or relocate files to free up space.</p>"},{"location":"storage/#shared-group-directories","title":"Shared Group Directories","text":"<ul> <li>Location: <code>/data/groups/&lt;groupname&gt;</code></li> <li>Quota: 1 TB per group</li> </ul> <p>Groups and labs each have a shared directory that can be used by all members of the group. These directories facilitate collaborative work, such as sharing datasets, code, and results within the group.</p>"},{"location":"storage/#quotas-and-limits","title":"Quotas and Limits","text":"<p>All quotas are enforced automatically. If you exceed your quota, you may be unable to write new data until you remove or relocate files.</p> <ul> <li>User Home Quota: 500 GB</li> <li>Group Shared Quota: 1 TB</li> </ul> <p>Need more storage? Request a quota increase.</p>"},{"location":"storage/hugging-face-model-cache/","title":"Storage","text":""},{"location":"storage/hugging-face-model-cache/#how-to-use-the-global-hugging-face-model-cache","title":"How to use the global Hugging Face model cache","text":"<p>The CAIS cluster provides a global Hugging Face model cache to facilitate efficient access to popular resources without affecting your file system quota. This cache is maintained by the CAIS cluster administrators and is regularly updated with frequently used models. Below is a guide on how to utilize this resource.</p>"},{"location":"storage/hugging-face-model-cache/#accessing-the-global-cache","title":"Accessing the Global Cache","text":"<p>The global Hugging Face cache is located at <code>/data/huggingface/</code>. You can use the cache by either setting the <code>cache_dir</code> argument or by setting the <code>HF_HOME</code> environment variable. For example:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncache_dir = \"/data/huggingface/\"\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", cache_dir=cache_dir)\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n</code></pre>"},{"location":"storage/hugging-face-model-cache/#requesting-models-to-be-added-to-the-cache","title":"Requesting Models to be Added to the Cache","text":"<p>If you require a model that is not already cached, you can request it by posting in the #shared-models Slack channel. Please include the full name, such as <code>meta-llama/Meta-Llama-3.1-8B</code>. Requests are typically processed within 24 hours, subject to approval.</p> <p>You can view the list of models currently available in the global cache on our Shared Models Tracker.</p>"},{"location":"storage/hugging-face-model-cache/#cache-maintenance-and-updates","title":"Cache Maintenance and Updates","text":"<p>The global cache is updated regularly, with popular models added automatically. Models that have not been used for over 60 days will be removed.</p>"},{"location":"storage/hugging-face-model-cache/#troubleshooting","title":"Troubleshooting","text":"<p>Here are some common issues and how to resolve them:</p> Issue/Message Advice Model Not Found Double-check the model name for typos. Refer to the Shared Models Tracker for the correct path. Missing Dependencies Ensure all required Python packages are installed. Missing Token Obtain and configure the required authentication token for models that need it. Refer to Hugging Face docs. Need More Help? Reach out via the #shared-models-data Slack channel for further assistance. <p>Using Custom Models</p> <p>If you need to cache a custom model locally, feel free to make use of a local HF cache. However, be mindful of storage quotas and only use local caching when necessary.</p>"},{"location":"storage/request-additional-storage/","title":"Storage","text":""},{"location":"storage/request-additional-storage/#how-to-request-additional-filesystem-storage","title":"How to request additional filesystem storage","text":"<p>By default, all users of the cluster are limited to 500 GB of file system storage on the cluster. If you need more storage for your project, you can submit an application indicating how much additional storage you need and for how long. We are usually able to provide a decision within 2-3 days.</p>"},{"location":"blog/","title":"Blog","text":""}]}